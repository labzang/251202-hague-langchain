"""RAG ì²´ì¸ ì„¤ì • ë° ê´€ë¦¬."""

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import PGVector

from app.config import settings


def create_rag_chain(vectorstore: PGVector):
    """RAG (Retrieval-Augmented Generation) ì²´ì¸ ìƒì„±."""
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt = ChatPromptTemplate.from_template(
        """
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""
    )

    # ê²€ìƒ‰ê¸° ì„¤ì •
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    # LLM ì„¤ì • ë° RAG ì²´ì¸ êµ¬ì„±
    if settings.openai_api_key:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

        # ì‹¤ì œ RAG ì²´ì¸ êµ¬ì„±
        rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
    else:
        # ë”ë¯¸ RAG í•¨ìˆ˜ (OpenAI API í‚¤ê°€ ì—†ì„ ë•Œ)
        def dummy_rag_function(question: str) -> str:
            """OpenAI API í‚¤ê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ëŠ” ë”ë¯¸ RAG í•¨ìˆ˜."""
            docs = retriever.invoke(question)
            context = "\n".join([f"- {doc.page_content}" for doc in docs])

            return f"""ğŸ” ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤:
{context}

ğŸ’¡ ë”ë¯¸ ì‘ë‹µ: ìœ„ì˜ ë¬¸ì„œë“¤ì´ '{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì…ë‹ˆë‹¤.
ì‹¤ì œ AI ì‘ë‹µì„ ë°›ìœ¼ë ¤ë©´ OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.
í•˜ì§€ë§Œ ë²¡í„° ê²€ìƒ‰ ê¸°ëŠ¥ì€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆìŠµë‹ˆë‹¤!"""

        # RunnableLambdaë¡œ ë˜í•‘í•˜ì—¬ ì²´ì¸ê³¼ í˜¸í™˜ë˜ë„ë¡ í•¨
        rag_chain = RunnableLambda(dummy_rag_function)

    return rag_chain

